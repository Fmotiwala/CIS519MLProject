{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNET Segmentation.ipynb","provenance":[],"collapsed_sections":["t1SxXob9KhD8","i8ysgtMrKhD9"],"toc_visible":true},"kernelspec":{"display_name":"conda_pytorch_latest_p36","language":"python","name":"conda_pytorch_latest_p36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ohsHAXKhD4","outputId":"b0412682-0805-40c6-90da-e2abeec123d1"},"source":["#### Imports"]},{"cell_type":"code","metadata":{"id":"ow5HqfMHKhDw","executionInfo":{"status":"ok","timestamp":1619483582938,"user_tz":240,"elapsed":5863,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["import os\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","%matplotlib inline\n","\n","#!pip install tqdm\n","from tqdm import tqdm_notebook, tnrange\n","from itertools import chain\n","\n","from skimage.io import imread, imshow, concatenate_images\n","from skimage.transform import resize\n","from skimage.morphology import label\n","from sklearn.model_selection import train_test_split\n","\n","from pylab import imshow\n","\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch import save, load\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CggVdABKhD3","executionInfo":{"status":"ok","timestamp":1619483582939,"user_tz":240,"elapsed":5860,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["# Set some parameters\n","im_width = 256\n","im_height = 256\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydOZXELRKhD4"},"source":["#### Load the base image and mask"]},{"cell_type":"code","metadata":{"id":"UeEuAEolPN_T","executionInfo":{"status":"ok","timestamp":1619483582940,"user_tz":240,"elapsed":5856,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["train_dir = \"ND2L/t000.tif\"\n","mask_dir = \"ND2L/man_seg000.tif\""],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fDMxheSccMF4"},"source":["#### Dataset Class\n"]},{"cell_type":"code","metadata":{"id":"frpMluOwcLl4","executionInfo":{"status":"ok","timestamp":1619483582940,"user_tz":240,"elapsed":5851,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["import pandas as pd \n","import os \n","from skimage import io\n","import  csv\n","from torchvision import datasets, transforms\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class CellSeg(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","\n","    def __len__(self):\n","        return 100 # -------------\n","\n","    def transform(self, image, mask):\n","\n","        # Random crop\n","        i, j, h, w = transforms.RandomCrop.get_params(\n","            image, output_size=(512, 512))\n","        image = TF.crop(image, i, j, h, w)\n","        mask = TF.crop(mask, i, j, h, w)\n","\n","        # Random horizontal flipping\n","        if random.random() > 0.5:\n","            image = TF.hflip(image)\n","            mask = TF.hflip(mask)\n","\n","        # Random vertical flipping\n","        if random.random() > 0.5:\n","            image = TF.vflip(image)\n","            mask = TF.vflip(mask)\n","\n","        # Transform to tensor\n","        image = TF.to_tensor(image)\n","        mask = TF.to_tensor(mask)\n","        return image, mask\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, 't000.tif')\n","        mask_path = os.path.join(self.mask_dir, 'man_seg000.tif')\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask>0] = 255.0\n","        mask = Image.fromarray(mask)\n","\n","        x, y = self.transform(image, mask)\n","\n","        return x, y\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-X66LFxNcG_I"},"source":["#### Create Model"]},{"cell_type":"code","metadata":{"id":"j8qSMbwkFS0_","executionInfo":{"status":"ok","timestamp":1619483586760,"user_tz":240,"elapsed":9667,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","     \n","\n","class Unet(torch.nn.Module):\n","    def __init__(self, input_ch=3, out_ch=1, feat=[64, 128, 512]):\n","        super(Unet, self).__init__()\n","\n","        self.kernel_size = 3\n","        self.stride = 1\n","        self.padding_mode = 1\n","        self.bias = True\n","\n","        # Contracting\n","        self.contracting = nn.Sequential(\n","            \n","            nn.Conv2d(input_ch, feat[0], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[0]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[0], feat[0], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[0]),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(feat[0], feat[1], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[1]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[1], feat[1], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[1]),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(feat[1], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[2], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        )   \n","\n","\n","        # Fully Connected\n","        self.connected = nn.Sequential(\n","            nn.Conv2d(feat[2], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[2], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","\n","        #Expansive -->  Ver conv2d aca\n","        self.expansive = nn.Sequential(\n","            \n","            nn.ConvTranspose2d(feat[2],feat[2], kernel_size=2, stride=2),\n","            nn.Conv2d(feat[2], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[2], feat[2], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[2]),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.ConvTranspose2d(feat[1]*4,feat[1]*4, kernel_size=2, stride=2),\n","            nn.Conv2d(feat[1]*4, feat[1], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[1]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[1], feat[1], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[1]),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.ConvTranspose2d(feat[0]*2,feat[0]*2, kernel_size=2, stride=2),\n","            nn.Conv2d(feat[0]*2, feat[0], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[0]),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(feat[0], feat[0], self.kernel_size, self.stride, self.padding_mode, self.bias),\n","            nn.BatchNorm2d(feat[0]),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.final = nn.Conv2d(feat[0], out_ch, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        x = self.contracting(x)\n","        x = self.connected(x)\n","        x = self.expansive(x)\n","\n","        return self.final(x)\n","\n","def test():\n","    x = torch.randn((3, 1, 160, 160))\n","    model = Unet(input_ch=1, out_ch=1)\n","    preds = model(x)\n","    assert preds.shape == x.shape\n","\n","if __name__ == \"__main__\":\n","    test()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_5EqMQxvh-V"},"source":["#### Util Functions"]},{"cell_type":"code","metadata":{"id":"M0geATjKY83i","executionInfo":{"status":"ok","timestamp":1619483586765,"user_tz":240,"elapsed":9670,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","from collections import OrderedDict\n","import numpy as np\n","\n","\n","def summary(model, input_size, batch_size=-1, device=torch.device('cpu'), dtypes=None, verbose=True):\n","    result, params_info = summary_string(\n","        model, input_size, batch_size, device, dtypes)\n","    if verbose:\n","        print(result)\n","\n","    return params_info\n","\n","\n","def summary_string(model, input_size, batch_size=-1, device=torch.device('cpu'), dtypes=None):\n","    if dtypes == None:\n","        dtypes = [torch.FloatTensor]*len(input_size)\n","\n","    summary_str = ''\n","\n","    def register_hook(module):\n","        def hook(module, input, output):\n","            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n","            module_idx = len(summary)\n","\n","            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n","            summary[m_key] = OrderedDict()\n","            summary[m_key][\"input_shape\"] = list(input[0].size())\n","            summary[m_key][\"input_shape\"][0] = batch_size\n","            if isinstance(output, (list, tuple)):\n","                summary[m_key][\"output_shape\"] = [\n","                    [-1] + list(o.size())[1:] for o in output\n","                ]\n","            else:\n","                summary[m_key][\"output_shape\"] = list(output.size())\n","                summary[m_key][\"output_shape\"][0] = batch_size\n","\n","            params = 0\n","            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n","                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n","                summary[m_key][\"trainable\"] = module.weight.requires_grad\n","            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n","                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n","            summary[m_key][\"nb_params\"] = params\n","            \n","\n","        if (\n","            not isinstance(module, nn.Sequential)\n","            and not isinstance(module, nn.ModuleList)\n","        ):\n","            hooks.append(module.register_forward_hook(hook))\n","\n","    # multiple inputs to the network\n","    if isinstance(input_size, tuple):\n","        input_size = [input_size]\n","\n","    # batch_size of 2 for batchnorm\n","    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n","         for in_size, dtype in zip(input_size, dtypes)]\n","\n","    # create properties\n","    summary = OrderedDict()\n","    hooks = []\n","\n","    # register hook\n","    model.apply(register_hook)\n","\n","    # make a forward pass\n","    # print(x.shape)\n","    model(*x)\n","\n","    # remove these hooks\n","    for h in hooks:\n","        h.remove()\n","\n","    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\n","        \"Layer (type)\", \"Output Shape\", \"Param #\")\n","    summary_str += line_new + \"\\n\"\n","    summary_str += \"================================================================\" + \"\\n\"\n","    total_params = 0\n","    total_output = 0\n","    trainable_params = 0\n","    total_conv2d = 0\n","    total_linear = 0 \n","    for layer in summary:\n","        if 'conv2d' in layer.lower():\n","            total_conv2d += 1\n","        if 'linear' in layer.lower():\n","            total_linear += 1 \n","\n","        # input_shape, output_shape, trainable, nb_params\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(\n","            layer,\n","            str(summary[layer][\"output_shape\"]),\n","            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n","        )\n","        total_params += summary[layer][\"nb_params\"]\n","\n","        total_output += np.prod(summary[layer][\"output_shape\"])\n","        if \"trainable\" in summary[layer]:\n","            if summary[layer][\"trainable\"] == True:\n","                trainable_params += summary[layer][\"nb_params\"]\n","        summary_str += line_new + \"\\n\"\n","\n","    # assume 4 bytes/number (float on cuda).\n","    total_input_size = abs(np.prod(sum(input_size, ()))\n","                           * batch_size * 4. / (1024 ** 2.))\n","    total_output_size = abs(2. * total_output * 4. /\n","                            (1024 ** 2.))  # x2 for gradients\n","    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n","    total_size = total_params_size + total_output_size + total_input_size\n","\n","    summary_str += \"================================================================\" + \"\\n\"\n","    summary_str += \"Total Conv2d layers: {0:,}\".format(total_conv2d) + \"\\n\"\n","    summary_str += \"Total Linear layers: {0:,}\".format(total_linear) + \"\\n\"\n","    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n","    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n","    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n","                                                        trainable_params) + \"\\n\"\n","    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n","    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n","    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n","    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n","    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n","    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n","    # return summary\n","    return summary_str, {'total_params': total_params, \n","                         'total_trainable_params': trainable_params,\n","                         'total_conv2d': total_conv2d,\n","                         'total_linear': total_linear}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"VMFTLb1MZWQ1","jupyter":{"outputs_hidden":true},"executionInfo":{"status":"ok","timestamp":1619483587308,"user_tz":240,"elapsed":10135,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}},"outputId":"f3b66fb7-14d4-40f2-e461-2504b5397f35"},"source":["def runRamdomSeed():\n","    torch.manual_seed(0)\n","    np.random.seed(0)\n","    random.seed(0)\n","    # Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n","    # causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n","    torch.backends.cudnn.benchmark = False \n","\n","runRamdomSeed()\n","\n","def summarize_Unet():\n","    # Run randomseed here to ensure results are consistent\n","    runRamdomSeed()\n","    student_net = Unet()\n","\n","    # Investigate your network's layers\n","    # Compare the printed shape with what expected in the specification\n","    print(\"\\n========= Model summarization ============ \") \n","    student_net_info = summary(student_net, (3, 64, 64), device='cpu')\n","\n","\n","summarize_Unet()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n","========= Model summarization ============ \n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 64]           1,792\n","       BatchNorm2d-2           [-1, 64, 64, 64]             128\n","              ReLU-3           [-1, 64, 64, 64]               0\n","            Conv2d-4           [-1, 64, 64, 64]          36,928\n","       BatchNorm2d-5           [-1, 64, 64, 64]             128\n","              ReLU-6           [-1, 64, 64, 64]               0\n","         MaxPool2d-7           [-1, 64, 32, 32]               0\n","            Conv2d-8          [-1, 128, 32, 32]          73,856\n","       BatchNorm2d-9          [-1, 128, 32, 32]             256\n","             ReLU-10          [-1, 128, 32, 32]               0\n","           Conv2d-11          [-1, 128, 32, 32]         147,584\n","      BatchNorm2d-12          [-1, 128, 32, 32]             256\n","             ReLU-13          [-1, 128, 32, 32]               0\n","        MaxPool2d-14          [-1, 128, 16, 16]               0\n","           Conv2d-15          [-1, 512, 16, 16]         590,336\n","      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n","             ReLU-17          [-1, 512, 16, 16]               0\n","           Conv2d-18          [-1, 512, 16, 16]       2,359,808\n","      BatchNorm2d-19          [-1, 512, 16, 16]           1,024\n","             ReLU-20          [-1, 512, 16, 16]               0\n","        MaxPool2d-21            [-1, 512, 8, 8]               0\n","           Conv2d-22            [-1, 512, 8, 8]       2,359,808\n","      BatchNorm2d-23            [-1, 512, 8, 8]           1,024\n","             ReLU-24            [-1, 512, 8, 8]               0\n","           Conv2d-25            [-1, 512, 8, 8]       2,359,808\n","      BatchNorm2d-26            [-1, 512, 8, 8]           1,024\n","             ReLU-27            [-1, 512, 8, 8]               0\n","  ConvTranspose2d-28          [-1, 512, 16, 16]       1,049,088\n","           Conv2d-29          [-1, 512, 16, 16]       2,359,808\n","      BatchNorm2d-30          [-1, 512, 16, 16]           1,024\n","             ReLU-31          [-1, 512, 16, 16]               0\n","           Conv2d-32          [-1, 512, 16, 16]       2,359,808\n","      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n","             ReLU-34          [-1, 512, 16, 16]               0\n","  ConvTranspose2d-35          [-1, 512, 32, 32]       1,049,088\n","           Conv2d-36          [-1, 128, 32, 32]         589,952\n","      BatchNorm2d-37          [-1, 128, 32, 32]             256\n","             ReLU-38          [-1, 128, 32, 32]               0\n","           Conv2d-39          [-1, 128, 32, 32]         147,584\n","      BatchNorm2d-40          [-1, 128, 32, 32]             256\n","             ReLU-41          [-1, 128, 32, 32]               0\n","  ConvTranspose2d-42          [-1, 128, 64, 64]          65,664\n","           Conv2d-43           [-1, 64, 64, 64]          73,792\n","      BatchNorm2d-44           [-1, 64, 64, 64]             128\n","             ReLU-45           [-1, 64, 64, 64]               0\n","           Conv2d-46           [-1, 64, 64, 64]          36,928\n","      BatchNorm2d-47           [-1, 64, 64, 64]             128\n","             ReLU-48           [-1, 64, 64, 64]               0\n","           Conv2d-49            [-1, 1, 64, 64]              65\n","             Unet-50            [-1, 1, 64, 64]               0\n","================================================================\n","Total Conv2d layers: 15\n","Total Linear layers: 0\n","Total params: 15,669,377\n","Trainable params: 15,669,377\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 59.56\n","Params size (MB): 59.77\n","Estimated Total Size (MB): 119.38\n","----------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wtaYXk0UiPw7","executionInfo":{"status":"ok","timestamp":1619483587308,"user_tz":240,"elapsed":10132,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["def check_model_exist_by_name(model_name):\n","    if os.path.exists(model_name + \".pth\") and os.path.isfile(model_name + \".pth\"):\n","        print(f\"[Successly trained and saved!] Your model named {model_name}.pth has been saved! DO NOT change the file's name! Just include it in your submission files.\")\n","    else:\n","        print(f\"[Successly trained but failed to save!] Your model named {model_name}.pth has not been saved or saved in a diffrent name from what we expect!\")\n","        import glob\n","        pt_files = glob.glob(\"*.pth\")\n","        if pt_files:\n","            print(f\"---> Somehow you've saved models as, {pt_files} which is not what autograder expects!\")\n","            print(f\"---> We expect that the trained model to be saved exactly as {model_name}.pth\")\n","            print(f\"---> What you can do is to manually rename the trained model to be {model_name}.pth\")\n","        else:\n","            print(f\"---> We found no saved models in *.pth format at all! Manually check if your saved the models in other formats or they are not saved at all!\")\n","\n","\n","def save_model(model, name):\n","    if isinstance(model, Unet):\n","        return save(model.state_dict(), name + \".pth\")\n","    \n","    raise ValueError(\"model type '%s' not supported!\"%str(type(model)))\n","\n","\n","def load_model(name, device_name='cpu'):\n","\n","    if \".\" in name:\n","        name = name.split('.')[0]\n","        \n","    if name == \"Unet\":\n","        r = Unet()\n","    else:\n","        raise ValueError(f\"model {name} has not been supported! Check the spelling!\")\n","    r.load_state_dict(load(name + \".pth\", map_location=device_name))\n","    return r"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"DP8TysJxigVJ","executionInfo":{"status":"ok","timestamp":1619483587309,"user_tz":240,"elapsed":10130,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["def accuracy(outputs, labels):\n","    outputs_idx = outputs.max(1)[1].type_as(labels)\n","    return outputs_idx.eq(labels).float().mean()\n","\n","def predict(model, inputs, device='cpu'):\n","    inputs = inputs.to(device)\n","    logits = model(inputs)\n","    return F.softmax(logits, -1)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waS8eJ0ibziv"},"source":["#### Training"]},{"cell_type":"code","metadata":{"id":"BKq5YEP4b1RG","executionInfo":{"status":"ok","timestamp":1619483587309,"user_tz":240,"elapsed":10127,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["def load_data(dataset_path,mask_path, data_transforms=None, num_workers=0, batch_size=128):\n","    dataset = CellSeg(dataset_path,mask_path)\n","    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"bM-cxJ5-lKlc","executionInfo":{"status":"ok","timestamp":1619483587310,"user_tz":240,"elapsed":10126,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["class ClassificationLoss(torch.nn.Module):\n","    def forward(self, input, target):\n","        return torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(input, dim=-1), target)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgA7x3VGhHPH","executionInfo":{"status":"ok","timestamp":1619483587310,"user_tz":240,"elapsed":10123,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["class Args(object):\n","    pass\n","\n","args = Args();\n","\n","args.learning_rate = 0.0001\n","args.log_dir = './my_tensorboard_log_directory' \n","args.num_epochs = 5\n","args.dataset_path = 'ND2L/'\n","args.mask_path = 'ND2L/'\n","args.validation_path = 'ND2L/'\n","args.batch_size = 128"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCRn66CPhPq9","executionInfo":{"status":"ok","timestamp":1619483587773,"user_tz":240,"elapsed":10584,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["def train(args, model_name=\"Unet\"):\n","    \"\"\"\n","    @Brief: training your model. This should include the following items:\n","        - Initialize the model (already given). Only need to map the model to the device on which you would want to run the model on \n","                using the following syntax: \n","                model = model.to(device) \n","                where device = torch.device(<device_name>), \n","                i.e: device = torch.device(\"cuda:0\") or device = torech.device(\"cpu\")\n","                    \n","        - Initialize tensorboard summarizers (already given)\n","        - Initialize data loaders (you need to code up)\n","        - Initialize the optimizer (you need to code up. Type is of your choice)\n","        - Initialize the loss function (you should have coded up above)\n","        - A for loop to iterate through many epochs (up to your choice). In each epoch:\n","                - Iterate through every mini-batches (remember to map data and labels to the device that you would want to run the model on)\n","                        - Run the forward path\n","                        - Get loss\n","                        - Calculate gradients \n","                        - Update the model's parameters\n","                - Evaluate your model on the validation set\n","                - Save the model if the performance on the validation set is better using exactly the following line:\n","                        save_model(model, model_name) \n","                 \n","    @Inputs: \n","        Args: object of your choice to carry arguments that you want to use within your training function. \n","    @Output: \n","        No return is necessary here. \n","    \"\"\"\n","    # Do not touch the following lines\n","    # Initialize the model \n","    model = Unet()\n","\n","    #----------------------------------------\n","    \n","    data_loader = load_data(args.dataset_path, args.mask_path, data_transforms=None, num_workers=0, batch_size=args.batch_size)\n","    criterion = ClassificationLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n","\n","    prev_validation_loss = float('inf')\n","\n","    for epoch in range(args.num_epochs):\n","        for i, batch in enumerate(data_loader):\n","          [X_train, Y_train] = batch\n","          optimizer.zero_grad()\n","          outputs = model(X_train)\n","          loss = criterion(outputs, Y_train)\n","          loss.backward()\n","          optimizer.step()\n","          if i % 20 == 0: print(f'Completed Batch {i} of {int(21002/args.batch_size)}')\n","        print(f'Completed Epoch {epoch} of {args.num_epochs}')\n","\n","        val_data_loader = load_data(args.validation_path, batch_size=9001)\n","        for i, batch in enumerate(val_data_loader):\n","          optimizer.zero_grad()\n","          outputs = model(X_train)\n","          loss = criterion(outputs, Y_train)\n","\n","          print('Validation Loss', loss.item())\n","\n","          if loss.item() < prev_validation_loss:\n","            print(f'Less than previous loss of {prev_validation_loss}, saving model...')\n","            save_model(model, model_name)\n","            prev_validation_loss = loss.item()\n","          print()\n","\n","\n","    # save_model(model, model_name) \n","    # assert os.path.exists(model_name + \".pth\") and os.path.isfile(model_name + \".pth\"), f\"[Fail to save your model named {model_name}.pth!\"\n","    return model"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NT_Hl-WBh-Of","outputId":"06c0697f-5f17-4ba5-a0d8-da294e034f47"},"source":["model_name=\"Unet\"\n","unet_model = train(args, model_name=model_name)\n","    \n","# Make sure that the model you've trained above has already been saved! \n","# check_model_exist_by_name(model_name)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:132: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n","  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"t1SxXob9KhD8"},"source":["### Saving Images for Evaluation"]},{"cell_type":"code","metadata":{"id":"d3hEv-hbvh-d","executionInfo":{"status":"aborted","timestamp":1619483587778,"user_tz":240,"elapsed":10563,"user":{"displayName":"Dora Maria Racca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd2At6U3AXDnbPnbSLnHwVCg-iUl4OZdifeuNc=s64","userId":"17199157931598423690"}}},"source":["def image_source(image, mask):\n","\n","    # Random crop\n","    i, j, h, w = transforms.RandomCrop.get_params(\n","        image, output_size=(256, 256))\n","    image = TF.crop(image, i, j, h, w)\n","    mask = TF.crop(mask, i, j, h, w)\n","\n","    # Random horizontal flipping\n","    if random.random() > 0.5:\n","        image = TF.hflip(image)\n","        mask = TF.hflip(mask)\n","\n","    # Random vertical flipping\n","    if random.random() > 0.5:\n","        image = TF.vflip(image)\n","        mask = TF.vflip(mask)\n","\n","    return image, mask\n","\n","original_img = Image.open('ND2L/t009.tif')\n","original_msk = np.array(Image.open('ND2L/man_seg009.tif').convert(\"L\"), dtype=np.float32)\n","original_msk[original_msk>0] = 255.0\n","original_msk = Image.fromarray(original_msk)\n","\n","save_path = 'SavedImgs/'\n","\n","for i in range(10):\n","    image, mask = image_source(original_img, original_msk)\n","    pred = predict(unet_model, img)\n","    \n","    image.save(save_path+'im'+str(i)+'.tiff')\n","    mask.save(save_path+'mas'+str(i)+'.tiff')\n","    pred.save(save_path+'pred'+str(i)+'.tiff')\n"],"execution_count":null,"outputs":[]}]}