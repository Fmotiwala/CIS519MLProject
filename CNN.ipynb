{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c927ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random \n",
    "import os, math\n",
    "\n",
    "LABEL_NAMES = {'no':0, 'yes':1}\n",
    "\n",
    "LABEL_=['no','yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f91e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed.\n",
    "def runRamdomSeed():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    # Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n",
    "    # causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "runRamdomSeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82252020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Image.open('/home/ec2-user/SageMaker/Debug/0.tif').convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ba4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from skimage import io \n",
    "import csv as csv\n",
    "class SuperCellDataset(Dataset):\n",
    "    def __init__(self, image_path):\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.cell_info = pd.read_csv(image_path + 'labels.csv')\n",
    "        self.length = len(self.cell_info)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, id):\n",
    "      \n",
    "        direction = self.cell_info.iloc[id][\"Direction\"]\n",
    "        crop_coord = self.cell_info.iloc[id][\"Outline\"]\n",
    "        frame = str(self.cell_info.iloc[id][\"Frame_Num\"])\n",
    "              #print(frame)\n",
    "        try:\n",
    "            full_image_path = self.image_path + frame + \".tif\" \n",
    "        except: \n",
    "            full_image_path = self.image_path + frame + \".tiff\"\n",
    "        img = Image.open(full_image_path).convert('RGB')\n",
    "\n",
    "        crop_coord = crop_coord.replace('(','').replace(')','')\n",
    "        crop_coord = crop_coord.split(\",\")\n",
    "\n",
    "\n",
    "        direction = direction.replace('[', '').replace(']', '').strip()\n",
    "        direction = re.sub(' +', ' ', direction)\n",
    "\n",
    "        direction = direction.split(\" \")\n",
    "\n",
    "        left = int(crop_coord[0])\n",
    "        right = int(crop_coord[1])\n",
    "        top = int(crop_coord[2])\n",
    "        bottom = int(crop_coord[3])\n",
    "\n",
    "      # https://www.geeksforgeeks.org/python-crop-image-using-pillow/\n",
    "      #a 4-tuple defining the left, upper, right, and lower pixel coordinate.\n",
    "        img_cropped = img.crop((left,top,right,bottom))\n",
    "      #print(type(img_cropped))\n",
    "      #img_tensor = transforms.ToTensor()(img_cropped)\n",
    "      \n",
    "    \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64,64)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "      \n",
    "        try:\n",
    "            img_tensor = transform((img_cropped))\n",
    "        except ValueError:\n",
    "            img_tensor=torch.Tensor()\n",
    "\n",
    "        label = 0\n",
    "\n",
    "        x = int(direction[0])\n",
    "        y = int(direction[1])\n",
    " \n",
    "      \n",
    "        #if x > 0:\n",
    "        #    if y > 0:\n",
    "        #      label= 1\n",
    "        #    else:\n",
    "        #      label= 4\n",
    "        #elif x < 0:\n",
    "        #    if y > 0:\n",
    "        #      label = 2\n",
    "        #    else:\n",
    "        #      label = 3\n",
    "        \n",
    "        distance = ((x**2) + (y**2))**0.5\n",
    "        if distance == 0:\n",
    "            label = 0\n",
    "        #elif distance > 2:\n",
    "        #    label = 1\n",
    "        else:\n",
    "            label=1\n",
    "\n",
    "        return(img_tensor,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e667c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_data():\n",
    "    Path_to_your_data= '/home/ec2-user/SageMaker/Processed/'\n",
    "    \n",
    "    dataset = SuperCellDataset(Path_to_your_data)\n",
    "\n",
    "    f, axes = plt.subplots(3, len(LABEL_NAMES))\n",
    "\n",
    "    counts = [0]*len(LABEL_NAMES)\n",
    "\n",
    "    for img, label in dataset:\n",
    "        c = counts[label]\n",
    "\n",
    "        if c < 3:\n",
    "            ax = axes[c][label]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "            ax.axis('off')\n",
    "            ax.set_title(LABEL_[label])\n",
    "            counts[label] += 1\n",
    "        \n",
    "        if sum(counts) >= 3 * len(LABEL_NAMES):\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "visualize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c417ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoss(torch.nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        Compute mean(-log(softmax(input)_label))\n",
    "        @input:  torch.Tensor((B,C)), where B = batch size, C = number of classes\n",
    "        @target: torch.Tensor((B,), dtype=torch.int64)\n",
    "        @return:  torch.Tensor((,))\n",
    "        Hint: use torch.nn.functional.nll_loss and torch.nn.functional.log_softmax\n",
    "        More details: https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss).\n",
    "        \"\"\"\n",
    "        #print(target.shape)\n",
    "        neg_log = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(input), target=target, reduction='mean')\n",
    "\n",
    "        return neg_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net):\n",
    "    \"\"\"\n",
    "    Usage: net = Model()\n",
    "           net.apply(init_weights)\n",
    "    \"\"\"\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90d60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BinaryCellClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Task: create a model with:\n",
    "          2 convolutional layers, followed by 2 linear layers, the last of which should output the logits for each class.\n",
    "          Check the table given above (section 3.3.2) for more details in the specification.\n",
    "        \"\"\"\n",
    "        # Don't remove the following line. Otherwise, it would raise ```AttributeError: cannot assign module before Module.__init__() call``` exception ERROR!\n",
    "        super(BinaryCellClassifier, self).__init__() \n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        #self.conv1 = nn.Conv2d(3, 8, 4, 2)\n",
    "        #self.conv2 = nn.Conv2d(8, 16, 4, 2)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.fc1 = nn.Linear(3136, 128)\n",
    "        #self.dropout2 = nn.Dropout(0.1)\n",
    "        #self.fc2 = nn.Linear(128, 3)\n",
    "        \n",
    "        self.conv = torch.nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=5, stride=1,  padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels = 64, out_channels = 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batch = torch.nn.BatchNorm2d(32)\n",
    "        self.batch2 = torch.nn.BatchNorm2d(64)\n",
    "        self.batch3 = torch.nn.BatchNorm2d(512)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=5, stride=3)\n",
    "\n",
    "        self.drop = torch.nn.Dropout2d(0.25)\n",
    "\n",
    "        self.fc1_layer = torch.nn.Linear(512, 64)\n",
    "        self.fc2_layer = torch.nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        @Brief: This function takes as input a tensor x of size Bx3x64x64 \n",
    "        and outputs a \"logit\" tensor of size Bx6. Do not include a softmax layer \n",
    "        here because most of Pytorch's loss functions take \"logit\" as one of the inputs\n",
    "        while integrating log() with softmax() into a log_softmax() function.    \n",
    "        @Inputs: \n",
    "          x: torch.Tensor((B,3,64,64)) \n",
    "        @return: torch.Tensor((B,6))\n",
    "        @Note: After the 2nd Conv2d (and ReLU), the intermediate feature has the shape \n",
    "               ```B x 16 x 14 x 14```. Before putting it into layer 3 (Linear), \n",
    "               make sure to reshape this intermediate feature to ```B x 3136```.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        #x = self.conv1(x)\n",
    "        #x = F.relu(x)\n",
    "\n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout1(x)\n",
    "\n",
    "        #x = torch.reshape(x, (x.size(0), 3136))\n",
    "\n",
    "        #x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "\n",
    "        #output = self.fc2(x)\n",
    "        \n",
    "             \n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = torch.reshape(x, (x.size(0), -1))\n",
    "\n",
    "        x = self.fc1_layer(x)\n",
    "        output = self.fc2_layer(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "def test_BinaryCellClassifier():\n",
    "    # Run randomseed here to ensure results are consistent\n",
    "    runRamdomSeed()\n",
    "    student_net = BinaryCellClassifier()\n",
    "\n",
    "    # Investigate your network's layers\n",
    "    # Compare the printed shape with what expected in the specification\n",
    "    print(\"\\n========= Model summarization ============ \") \n",
    "    student_net_info = summary(student_net, (3, 64, 64), device='cpu')\n",
    "\n",
    "  \n",
    "    # Check the number of Conv2d layers and Linear layers\n",
    "    total_conv2d = student_net_info['total_conv2d']\n",
    "    total_linear = student_net_info['total_linear']\n",
    "    if total_conv2d != 2:\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 2 Conv2d layers!, but yours consists of: {total_conv2d} Conv2d layers\")\n",
    "        return \n",
    "    if total_linear != 2:\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 2 Linear layers!, but yours consists of: {total_linear} Linear layers\")\n",
    "        return \n",
    "\n",
    "    # Check total number of parameters\n",
    "    total_parmas = student_net_info['total_params']\n",
    "    if total_parmas != 404766:\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 404,766 parameters!, but yours consists of: {total_parmas} parameters\")\n",
    "        print(f\"Check kernel size, stride, no of input features, no of output featurs of your Conv2d layer,\")\n",
    "        print(f\"\\t and no of input features, no of output featurs of your Linear layer\")\n",
    "        return \n",
    "\n",
    "    # Initialize weights for this network\n",
    "    student_net.apply(init_weights)\n",
    "\n",
    "    # Give the network a dummy input of size 2x3x64x64\n",
    "    batch_size  = 2\n",
    "    dummy_x  = torch.randn([batch_size, 3, 64, 64])\n",
    "    logit    = student_net(dummy_x) \n",
    "\n",
    "    print(\"========= Run Model with Dummy Input ============\\n\") \n",
    "    # Check type of the output logit\n",
    "    if not torch.is_tensor(logit):\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return a tensor!, but yours returns: {type(logit)}\")\n",
    "        return \n",
    "\n",
    "    # Check size of the output logit\n",
    "    if logit.size() != torch.Size([2,6]):\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return a tensor of size torch.Size([2,6])!, but your return's size: {logit.size()}\")\n",
    "        print(f\"Make sure that the number of layers, the layers, number of features, kernel size ... all are correct!\")\n",
    "        return\n",
    "  \n",
    "    # Check value\n",
    "    expected_logit = torch.tensor([[-1.7501, -2.2915,  0.2061, -0.9353,  0.5314,  1.1240],\n",
    "                                   [-1.3631, -1.3827,  1.7212, -0.1081, -1.3976,  2.3925]])\n",
    "\n",
    "    if torch.norm(logit - expected_logit) > 1e-3:\n",
    "        print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return \\n {expected_logit}\")\n",
    "        print(f\"However, yours returns: \\n{logit}\")\n",
    "        print(f\"Make sure that the number of layers, the layers, number of features, kernel size ... all are correct!\")\n",
    "        return\n",
    "\n",
    "    # Check to see if we can do backpropagation\n",
    "    base_loss_obj  = torch.nn.CrossEntropyLoss(reduction='mean')  \n",
    "    dummy_target   = torch.randint(0, 6, [2])\n",
    "    loss           =  base_loss_obj(logit, dummy_target)\n",
    "    loss.backward()\n",
    "    print(f\"Target: {dummy_target}\")\n",
    "    print(f\"Logit: {logit}\")\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------\") \n",
    "    print(\"\\n[SUCCESSFUL!] Congrats! Your implementation of ClassificationLoss looks correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc477e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL!\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    outputs_idx = outputs.max(1)[1].type_as(labels)\n",
    "    return outputs_idx.eq(labels).float().mean()\n",
    "\n",
    "def predict(model, inputs, device='cpu'):\n",
    "    inputs = inputs.to(device)\n",
    "    logits = model(inputs)\n",
    "    return F.softmax(logits, -1)\n",
    "\n",
    "def draw_bar(axis, preds, labels=None):\n",
    "    y_pos = np.arange(6)\n",
    "    axis.barh(y_pos, preds, align='center', alpha=0.5)\n",
    "    axis.set_xticks(np.linspace(0, 1, 10))\n",
    "    \n",
    "    if labels:\n",
    "        axis.set_yticks(y_pos)\n",
    "        axis.set_yticklabels(labels)\n",
    "    else:\n",
    "        axis.get_yaxis().set_visible(False)\n",
    "    \n",
    "    axis.get_xaxis().set_visible(False)\n",
    "\n",
    "def visualize_predictions(model=None, model_name=None, device_name='cpu'):\n",
    "  \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = load_model(model_name, device_name)\n",
    "    \n",
    "    # Get the device \n",
    "    if device_name is not None:\n",
    "        device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "    validation_image_path='/home/ec2-user/SageMaker/Processed_Valid/' #enter the path \n",
    "\n",
    "    dataset = SuperCellDataset(image_path=validation_image_path)\n",
    "\n",
    "    f, axes = plt.subplots(2, 6)\n",
    "\n",
    "    idxes = np.random.randint(0, len(dataset), size=6)\n",
    "\n",
    "    for i, idx in enumerate(idxes):\n",
    "        img, label = dataset[idx]\n",
    "        preds = predict(model, img[None], device=device).detach().cpu().numpy()\n",
    "\n",
    "        axes[0, i].imshow(TF.to_pil_image(img))\n",
    "        axes[0, i].axis('off')\n",
    "        draw_bar(axes[1, i], preds[0], LABEL_ if i == 0 else None)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddc87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    img = [item[0] for item in data]\n",
    "    label = [item[1] for item in data]\n",
    "\n",
    "    #zipped = zip(img, label)\n",
    "    \n",
    "    return [img, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907d3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, data_transforms=None, num_workers=0, batch_size=128):\n",
    "    dataset = SuperCellDataset(dataset_path)\n",
    "    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d563e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    pass\n",
    "\n",
    "args = Args();\n",
    "# Add attributes to args here, such as:\n",
    "args.learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12a266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, actual):\n",
    "  assert len(pred) == len(actual)\n",
    "\n",
    "  total = len(actual)\n",
    "  _, predicted = torch.max(pred.data, 1)\n",
    "  correct = (predicted == actual).sum().item()\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f800bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from torch import load\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    if isinstance(model, BinaryCellClassifier):\n",
    "        return save(model.state_dict(), name + \".pth\")\n",
    "    \n",
    "    raise ValueError(\"model type '%s' not supported!\"%str(type(model)))\n",
    "\n",
    "\n",
    "def load_model(name, device_name='cpu'):\n",
    "    \"\"\"\n",
    "    @Brief: load a model saved in the \".pth\" or \".pt\" formats\n",
    "    @Inputs:\n",
    "        name (str): name of the model (without the extension)\n",
    "        device_name (str): name of the device i.e: 'cpu', 'cuda:0', that you would want to run the model on.\n",
    "    @Outputs:\n",
    "        r (nn.Module): a Pytorch model of either \"BasicCNNClassifier\" or \"MyBestCNNClassifier\" (depend on \"name\" input) \n",
    "            with pretrained wieghts.\n",
    "    \"\"\"\n",
    "    # In case students set input name = \"*.pth\" \n",
    "    if \".\" in name:\n",
    "        name = name.split('.')[0]\n",
    "        \n",
    "    if name == \"BinaryCellClassifier\":\n",
    "        r = BasicCNNClassifier()\n",
    "    elif name == \"MyBestCNNClassifier\":\n",
    "        r = MyBestCNNClassifier()\n",
    "    else:\n",
    "        raise ValueError(f\"model {name} has not been supported! Check the spelling!\")\n",
    "    r.load_state_dict(load(name + \".pth\", map_location=device_name))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa23b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model_name=\"MyBestCNNClassifier\"):\n",
    "    \"\"\"\n",
    "    @Brief: training your model. This should include the following items:\n",
    "        - Initialize the model (already given). Only need to map the model to the device on which you would want to run the model on \n",
    "                using the following syntax: \n",
    "                model = model.to(device) \n",
    "                where device = torch.device(<device_name>), \n",
    "                i.e: device = torch.device(\"cuda:0\") or device = torech.device(\"cpu\")\n",
    "                    \n",
    "        - Initialize tensorboard summarizers (already given)\n",
    "        - Initialize data loaders (you need to code up)\n",
    "        - Initialize the optimizer (you need to code up. Type is of your choice)\n",
    "        - Initialize the loss function (you should have coded up above)\n",
    "        - A for loop to iterate through many epochs (up to your choice). In each epoch:\n",
    "                - Iterate through every mini-batches (remember to map data and labels to the device that you would want to run the model on)\n",
    "                        - Run the forward path\n",
    "                        - Get loss\n",
    "                        - Calculate gradients \n",
    "                        - Update the model's parameters\n",
    "                - Evaluate your model on the validation set\n",
    "                - Save the model if the performance on the validation set is better using exactly the following line:\n",
    "                        save_model(model, model_name) \n",
    "                 \n",
    "    @Inputs: \n",
    "        Args: object of your choice to carry arguments that you want to use within your training function. \n",
    "    @Output: \n",
    "        No return is necessary here. \n",
    "    \"\"\"\n",
    "    model = BinaryCellClassifier()\n",
    "\n",
    "    #device=torch.device('cpu')\n",
    "    #model=model.to(device)\n",
    "\n",
    "    train=load_data('/home/ec2-user/SageMaker/Processed/')\n",
    "    val = load_data('/home/ec2-user/SageMaker/Processed_Valid/')\n",
    "    optimizer=torch.optim.Adam(model.parameters())\n",
    "    loss=ClassificationLoss()\n",
    "    model.apply(init_weights)\n",
    "    epochs=100\n",
    "\n",
    "    acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "    # clearing the Gradients of the model parameters\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "      \n",
    "      for (X,Y) in train:\n",
    "        if (len(X) !=0):\n",
    "            #X = np.expand_dims(X, 1)\n",
    "                    \n",
    "            #X = np.squeeze(X)\n",
    "                    \n",
    "            #X = transforms.ToTensor()(X)\n",
    "            #x_it = x_it.unsqueeze(1)\n",
    "            #print(x_it)\n",
    "            #print(X.shape)\n",
    "            output_train = model.forward(X)\n",
    "            #print(output_train.shape)\n",
    "            #Y = Y.squeeze(1)\n",
    "            #Y = Y.unsqueeze(1).unsqueeze(1)\n",
    "            loss_train = loss(output_train, Y)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "      preds = []\n",
    "      labels = []\n",
    "      for (X,Y) in val:\n",
    "        if (len(X) != 0):\n",
    "            #print(X.shape)\n",
    "            y_pred = torch.argmax(model(X), dim = 1).tolist()\n",
    "            y_pred = map(int, y_pred)\n",
    "            preds.extend(list(y_pred))\n",
    "            labels.extend(Y.tolist())\n",
    "      new_acc = np.sum(np.array(preds) == np.array(labels))/len(np.array(preds))\n",
    "      if (new_acc > acc):\n",
    "        acc = new_acc\n",
    "        save_model(model, model_name)\n",
    "      #print(preds)\n",
    "      #print(labels)  \n",
    "      print('Epoch : ',e+1, '\\t', 'accuracy :', acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266a2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t accuracy : 0.9182968929804373\n",
      "Epoch :  2 \t accuracy : 0.9182968929804373\n",
      "Epoch :  3 \t accuracy : 0.9182968929804373\n",
      "Epoch :  4 \t accuracy : 0.9182968929804373\n",
      "Epoch :  5 \t accuracy : 0.9182968929804373\n",
      "Epoch :  6 \t accuracy : 0.9182968929804373\n",
      "Epoch :  7 \t accuracy : 0.9182968929804373\n",
      "Epoch :  8 \t accuracy : 0.9182968929804373\n"
     ]
    }
   ],
   "source": [
    "train(args, model_name=\"BinaryCellClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca2f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad85a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
